{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Latency benchmark for Context Caching with the Gemini API\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/tvaroska/samples/blob/main/notebook/timing_caching.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Ftvaroska%2Fsamples%2Fmain%2Fnotebooks%2Ftiming_caching.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/tvaroska/samples/notebooks/timing_caching.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/tvaroska/samples/blob/main/notebooks/timing_caching.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Boris Tvaroska](https://github.com/tvaroska)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Context Caching\n",
    "\n",
    "The Gemini API provides the context caching feature for developers to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model. This feature can help reduce the number of tokens sent to the model, thereby lowering the cost of requests that contain repeat content with high input token counts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "In this we will showcase latency improvement of caching large files before the requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHwJCyNF6u0O"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mginH0QC6u0O"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "from vertexai.preview.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XdpfXqpJ-NNj"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-1.5-pro-001\"  # @param {type:\"string\"}\n",
    "\n",
    "RUNS = 10 # How many requests to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UmJA6AvVujyZ"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "prompt = Part.from_text(\"What is the research goal shared by these research papers?\")\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cache_model = GenerativeModel(model_name=MODEL_ID, system_instruction=system_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1408eab9a82c448a80ebebef6da97b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cached version generation time: 30.159632882406004\n"
     ]
    }
   ],
   "source": [
    "no_cache_generate = []\n",
    "\n",
    "for _ in tqdm(range(RUNS)):\n",
    "    start = timer()\n",
    "    no_cache_model.generate_content(contents = [prompt] + contents)\n",
    "    end = timer()\n",
    "\n",
    "    no_cache_generate.append(end - start)\n",
    "\n",
    "avg_no_cache = float(sum(no_cache_generate) / len(no_cache_generate))\n",
    "print(f\"Non-cached version generation time: {avg_no_cache}\") # Time in seconds, e.g. 5.38091952400282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached version store time: 24.736746133014094\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=MODEL_ID,\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(minutes=60),\n",
    ")\n",
    "\n",
    "end = timer()\n",
    "\n",
    "cache_storing = end - start\n",
    "\n",
    "print(f\"Cached version store time: {cache_storing}\") # Time in seconds, e.g. 5.38091952400282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EPVyJIW1BaVj"
   },
   "outputs": [],
   "source": [
    "cached_model = GenerativeModel.from_cached_content(cached_content=cached_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is usefull for multiple interactions with content - we will do 2 questions to the same documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5dSDogewDAHB"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495cd4d80df4abdb1fe90473f7e9ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached version generation time: 28.236969568999484\n"
     ]
    }
   ],
   "source": [
    "cache_generate = []\n",
    "\n",
    "for _ in tqdm(range(RUNS)):\n",
    "    start = timer()\n",
    "\n",
    "    response = cached_model.generate_content(\n",
    "        \"What is the research goal shared by these research papers?\"\n",
    "    )\n",
    "\n",
    "    end = timer()\n",
    "    cache_generate.append(end - start)\n",
    "\n",
    "avg_cache = float(sum(cache_generate) / len(cache_generate))\n",
    "print(f\"Cached version generation time: {cache_generate[0]}\") # Time in seconds, e.g. 5.38091952400282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XGzgTk6YzgSt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CachedContent : projects/745535691203/locations/us-central1/cachedContents/6965414958053982208\n",
      "Delete cache time: 0.12102052301634103\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "cached_content.delete()\n",
    "\n",
    "end = timer()\n",
    "cache_delete = end - start\n",
    "print(f\"Delete cache time: {cache_delete}\") # Time in seconds, e.g. 5.38091952400282"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate answer without cache : [30.08364355802769, 28.209958852035925, 33.597342441033106, 39.158240033022594, 26.773604198999237, 26.96953611599747, 26.357575650967192, 30.852880140999332, 32.19091235694941, 27.40263547602808]\n",
      "--------------------------------------------------\n",
      "Time to submit documents to cache     : 24.736746133014094\n",
      "Time to generate answer with cache    : [28.236969568999484, 28.84433871298097, 39.46096307603875, 23.559182040044107, 22.769824632967357, 30.726974983001128, 26.36174183798721, 30.48225156898843, 42.67605299997376, 22.163112573034596]\n",
      "Time to delete cache                  : 0.12102052301634103\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to generate answer without cache : {no_cache_generate}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Time to submit documents to cache     : {cache_storing}\")\n",
    "print(f\"Time to generate answer with cache    : {cache_generate}\")\n",
    "print(f\"Time to delete cache                  : {cache_delete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_context_caching.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
