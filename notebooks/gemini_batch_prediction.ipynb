{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9",
    "tags": []
   },
   "source": [
    "# Gemini Batch Prediction\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/tvaroska/gemini-batch/blob/main/gemini_batch_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Ftvaroska%2Fgemini-batch%2Fmain%2Fgemini_batch_prediction.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/tvaroska/gemini-batch/blob/main/gemini_batch_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/tvaroska/gemini-batch/blob/main/gemini_batch_prediction.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author | [Boris Tvaroska](https://github.com/tvaroska)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHPv2myT2cx"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Vertex AI Gemini Batch Inference API\n",
    "\n",
    "For more information, see the [Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrkcqHrrwMAo"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Vertex AI Gemini Batch inference API with the Bigquery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9nEPojogw-g"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "- Bigquery\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11Gu7qNgx1p"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform google-cloud-bigquery pydantic tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRvKdaPDTznN",
    "outputId": "154a71b5-f302-4f53-ed2f-b3e5fef9195b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "DATASET = 'batch' # @param {type:\"string\"}\n",
    "PRO_MODEL = 'gemini-1.5-pro-001' # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TABLE = f'{PROJECT_ID}.{DATASET}.raw'\n",
    "SOURCE_TABLE = f'{PROJECT_ID}.{DATASET}.batch'\n",
    "TARGET_TABLE = f'{PROJECT_ID}.{DATASET}.result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tenacity import retry, wait_exponential, retry_if_exception_type\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from google.cloud.exceptions import Conflict, NotFound\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
    "from vertexai.batch_prediction._batch_prediction import BatchPredictionJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client()\n",
    "pro = GenerativeModel(model_name=PRO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "- generate_content = retry for online API for quota limitation\n",
    "- flatten_openapi = modify pydantic openAPI schema for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(retry = retry_if_exception_type(ResourceExhausted), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def generate_content(model, prompt, generation_config=None):\n",
    "    return model.generate_content(prompt, generation_config = generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/tvaroska/20362bd56a5060f1bc21933bd4fee657\n",
    "def flatten_openapi(schema):\n",
    "    try:\n",
    "        defs = {}\n",
    "\n",
    "        # Cover recursive submodels\n",
    "        for key, value in schema['$defs'].items():\n",
    "            replacement = value\n",
    "\n",
    "            for pkey in value['properties']:\n",
    "                if '$ref' in value['properties'][pkey]:\n",
    "                    replacement['properties'][pkey] = defs[value['properties'][pkey]['$ref']]\n",
    "                elif 'items' in value['properties'][pkey] and '$ref' in value['properties'][pkey]['items']:\n",
    "                    replacement['properties'][pkey]['items'] = defs[value['properties'][pkey]['items']['$ref']]\n",
    "            defs[f'#/$defs/{key}'] = replacement\n",
    "    except KeyError:\n",
    "        return schema\n",
    "\n",
    "    for key in schema['properties']:\n",
    "        # Replace direct ussage of submodel\n",
    "        if '$ref' in schema['properties'][key]:\n",
    "            ref = schema['properties'][key]['$ref']\n",
    "            schema['properties'][key] = defs[ref]\n",
    "        # Replace list of submodels\n",
    "        elif 'items' in schema['properties'][key]:\n",
    "            if '$ref' in schema['properties'][key]['items']:\n",
    "                ref = schema['properties'][key]['items']['$ref']\n",
    "                schema['properties'][key]['items'] = defs[ref]\n",
    "\n",
    "    del schema['$defs']\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BQ dataset and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "\n",
    "dataset = bigquery.Dataset(f'{PROJECT_ID}.{DATASET}')\n",
    "dataset.location = LOCATION\n",
    "\n",
    "bq.create_dataset(dataset, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to store transcripts\n",
    "\n",
    "raw_table = bigquery.Table(\n",
    "    table_ref=RAW_TABLE,\n",
    "    schema=[\n",
    "        bigquery.SchemaField('id', 'INTEGER', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('agent', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('customer', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('transcript', 'RECORD', mode='REPEATED', fields = [\n",
    "            bigquery.SchemaField('role', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('content', 'STRING', mode='REQUIRED')\n",
    "        ])\n",
    "    ]\n",
    ")\n",
    "\n",
    "try:\n",
    "    bq.create_table(raw_table)            \n",
    "except Conflict:\n",
    "    # Table exists, delete everything\n",
    "    bq.query_and_wait(f'DELETE FROM {RAW_TABLE} WHERE True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to prepare prompt and parameters for Gemini\n",
    "\n",
    "source_table = bigquery.Table(\n",
    "    table_ref=SOURCE_TABLE,\n",
    "    schema=[\n",
    "        bigquery.SchemaField('id', 'INTEGER', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('request', 'JSON', mode='REQUIRED')\n",
    "    ]\n",
    ")\n",
    "try:\n",
    "    bq.create_table(source_table)\n",
    "except Conflict:\n",
    "    # Table exists, delete everything\n",
    "    bq.query_and_wait(f'DELETE FROM {SOURCE_TABLE} WHERE True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete table with Gemini outputs\n",
    "\n",
    "try:\n",
    "    bq.delete_table(TARGET_TABLE)\n",
    "except NotFound:\n",
    "    pass # Table does not have to exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic call transcripts\n",
    "\n",
    "Use Gemini 1.5 Pro online API calls with controled generated output to create hypothetical call transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = ['credit card', 'car insurance']\n",
    "issues = ['unknown fee', 'lost password']\n",
    "customers = ['angry and hard to understand', 'calm']\n",
    "agents = ['helpfull', 'profesional, but distant']\n",
    "outputs = ['solved', 'escalated to supervisor']\n",
    "\n",
    "count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptLine(BaseModel):\n",
    "    role: Literal['agent', 'customer']\n",
    "    content: str\n",
    "\n",
    "class Transcript(BaseModel):\n",
    "    id: int\n",
    "    agent: str = Field(description = 'Name of the agent, first and last name')\n",
    "    customer: str = Field(description = 'Name of the customer, first and last name')\n",
    "    transcript: List[TranscriptLine]\n",
    "\n",
    "raw_schema = flatten_openapi(Transcript.schema())\n",
    "\n",
    "synthetic_config = GenerationConfig(temperature=2, response_mime_type='application/json', response_schema=raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1\n",
    "transcripts = []\n",
    "\n",
    "total = len(products) * len(issues) * len(customers) * len(agents) * len(outputs) * count\n",
    "\n",
    "with tqdm(total = total) as pb:\n",
    "    for product in products:\n",
    "        for issue in issues:\n",
    "            for customer in customers:\n",
    "                for agent in agents:\n",
    "                    for output in outputs:\n",
    "                        for _ in range(count):\n",
    "                            prompt = f'Generate an example for call center transcripts. Call is about {issue} with {product} product. Customer is {customer} and agent is {agent}. Output of the call is {output}. Call id = {id}'\n",
    "                            response = generate_content(pro, prompt, generation_config=synthetic_config)\n",
    "                            id += 1\n",
    "                            try:\n",
    "                                transcripts.append(Transcript.parse_raw(response.candidates[0].content.parts[0].text).dict())\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                            pb.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = bq.load_table_from_json(transcripts, RAW_TABLE)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "Use BQ SQL to transform transcripts in RAW_TABLE into request format for Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    category: str = Field(description='Create apropriate category')\n",
    "    product: Literal['product', 'oder', 'receipt', 'other']\n",
    "    sentiment: Literal['positive', 'neutral', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_query = f\"\"\"INSERT INTO {SOURCE_TABLE}(id, request)\n",
    "  (\n",
    "SELECT id, JSON_OBJECT(ARRAY_AGG(k), ARRAY_AGG(v)) as request FROM\n",
    "(\n",
    "SELECT id, k, v FROM \n",
    "(\n",
    "  (\n",
    "    SELECT 'generationConfig' as k, JSON_OBJECT('temperature', '0.5', 'responseMimeType', 'application/json', 'responseSchema', PARSE_JSON('{Response.schema_json()}')) as v UNION ALL\n",
    "    SELECT 'system_instruction',  JSON_OBJECT('parts', JSON_ARRAY(JSON_OBJECT('text', 'You are analyzing customer support calls from the call center. Clasify transcript.')))\n",
    "  ) CROSS JOIN (SELECT id FROM `boris001.batch.raw`)\n",
    ") UNION ALL\n",
    "      SELECT id, 'contents', JSON_ARRAY(JSON_OBJECT('role', 'user', 'parts', JSON_ARRAY(JSON_OBJECT('text', conversation)))) FROM\n",
    "      (\n",
    "        SELECT id, STRING_AGG(line, '\\\\n') as conversation FROM (SELECT id, CONCAT(role, ':', content) as line FROM {RAW_TABLE}, UNNEST(transcript)) GROUP BY id)\n",
    "      )\n",
    "    GROUP BY id\n",
    "  )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq.query_and_wait(transform_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch prediction\n",
    "\n",
    "and wait for operation to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = BatchPredictionJob.submit(\n",
    "    source_model='gemini-1.5-pro-001',\n",
    "    input_dataset=f'bq://{SOURCE_TABLE}',\n",
    "    output_uri_prefix=f'bq://{TARGET_TABLE}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while not batch_job.has_ended:\n",
    "    batch_job.refresh()\n",
    "    print(f\"Batch job state: {batch_job.state}\")\n",
    "    time.sleep(30)\n",
    "print(f\"Batch job state: {batch_job.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the output in structured format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bq.query_and_wait(f\"SELECT id, JSON_VALUE(text.category) as category, JSON_VALUE(text.product) as product, JSON_VALUE(text.sentiment) as sentiment FROM (SELECT id, PARSE_JSON(JSON_VALUE(response.candidates[0].content.parts[0].text)) as text FROM `{TARGET_TABLE}`)\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
